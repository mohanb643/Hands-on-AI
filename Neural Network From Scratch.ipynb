{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae52d38-5e45-4546-8f9d-1a4ccae46f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Neural Network: Input=2, Hidden=4, Output=1\n",
      "\n",
      "Starting training for 50000 epochs with LR=0.1...\n",
      "Epoch 0/50000 - Loss: 0.250001\n",
      "Epoch 5000/50000 - Loss: 0.250000\n",
      "Epoch 10000/50000 - Loss: 0.250000\n",
      "Epoch 15000/50000 - Loss: 0.250000\n",
      "Epoch 20000/50000 - Loss: 0.250000\n",
      "Epoch 25000/50000 - Loss: 0.250000\n",
      "Epoch 30000/50000 - Loss: 0.250000\n",
      "Epoch 35000/50000 - Loss: 0.250000\n",
      "Epoch 40000/50000 - Loss: 0.250000\n",
      "Epoch 45000/50000 - Loss: 0.250000\n",
      "Epoch 49999/50000 - Loss: 0.250000\n",
      "Training complete.\n",
      "\n",
      "--- Final Results ---\n",
      "Input (X):\n",
      " [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "True Labels (y):\n",
      " [0 1 1 0]\n",
      "Predicted Probabilities (A2):\n",
      " [0.50000605 0.50000046 0.49999957 0.49999397]\n",
      "Final Predictions:\n",
      " [1 1 0 0]\n",
      "\n",
      "Final Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"\n",
    "    A simple two-layer Neural Network (Input -> Hidden -> Output) implemented from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for the network layers.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of features in the input data.\n",
    "            hidden_size (int): Number of neurons in the hidden layer.\n",
    "            output_size (int): Number of neurons in the output layer (1 for binary classification).\n",
    "        \"\"\"\n",
    "        print(f\"Initializing Neural Network: Input={input_size}, Hidden={hidden_size}, Output={output_size}\")\n",
    "        \n",
    "        # --- Weights and Biases Initialization ---\n",
    "        # W1: Weights for Input -> Hidden layer (input_size x hidden_size)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01 \n",
    "        # b1: Biases for Hidden layer (1 x hidden_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "\n",
    "        # W2: Weights for Hidden -> Output layer (hidden_size x output_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        # b2: Biases for Output layer (1 x output_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "        # Store layer sizes\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Cache for intermediate values needed during backpropagation\n",
    "        self.cache = {}\n",
    "\n",
    "    # --- Activation Function: Sigmoid ---\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function: g(z) = 1 / (1 + e^(-z))\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, a):\n",
    "        \"\"\"\n",
    "        Derivative of the sigmoid function: g'(a) = a * (1 - a)\n",
    "        where 'a' is the output of the sigmoid function.\n",
    "        \"\"\"\n",
    "        return a * (1 - a)\n",
    "\n",
    "    # --- Forward Propagation ---\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the network.\n",
    "        \n",
    "        X: Input data (m x input_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Input Layer to Hidden Layer\n",
    "        # Z1 = X * W1 + b1 (Linear combination)\n",
    "        self.cache['Z1'] = np.dot(X, self.W1) + self.b1\n",
    "        \n",
    "        # A1 = sigmoid(Z1) (Activation)\n",
    "        self.cache['A1'] = self.sigmoid(self.cache['Z1'])\n",
    "        \n",
    "        # 2. Hidden Layer to Output Layer\n",
    "        # Z2 = A1 * W2 + b2 (Linear combination)\n",
    "        self.cache['Z2'] = np.dot(self.cache['A1'], self.W2) + self.b2\n",
    "        \n",
    "        # A2 = sigmoid(Z2) (Output prediction)\n",
    "        self.cache['A2'] = self.sigmoid(self.cache['Z2'])\n",
    "        \n",
    "        # A2 is the final predicted output\n",
    "        return self.cache['A2']\n",
    "\n",
    "    # --- Backward Propagation ---\n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs the backward pass to calculate gradients and update weights/biases.\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Input data.\n",
    "            y (np.array): True labels.\n",
    "            output (np.array): Predicted output from forward pass (A2).\n",
    "            learning_rate (float): The step size for gradient descent.\n",
    "        \"\"\"\n",
    "        m = X.shape[0] # Number of samples\n",
    "\n",
    "        # 1. Output Layer Gradient (dZ2)\n",
    "        # Error (difference between actual and predicted)\n",
    "        error = output - y.reshape(-1, 1)\n",
    "        \n",
    "        # Derivative of cost w.r.t Z2 (using derivative of sigmoid at A2)\n",
    "        dZ2 = error * self.sigmoid_derivative(output)\n",
    "        \n",
    "        # 2. Calculate gradients for W2 and b2\n",
    "        # dW2 = (1/m) * A1.T * dZ2\n",
    "        dW2 = (1 / m) * np.dot(self.cache['A1'].T, dZ2)\n",
    "        # db2 = (1/m) * sum(dZ2, axis=0)\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "        # 3. Hidden Layer Gradient (dZ1)\n",
    "        # Delta for hidden layer: propagate error backwards through W2\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        \n",
    "        # Derivative of cost w.r.t Z1 (using derivative of sigmoid at A1)\n",
    "        dZ1 = dA1 * self.sigmoid_derivative(self.cache['A1'])\n",
    "\n",
    "        # 4. Calculate gradients for W1 and b1\n",
    "        # dW1 = (1/m) * X.T * dZ1\n",
    "        dW1 = (1 / m) * np.dot(X.T, dZ1)\n",
    "        # db1 = (1/m) * sum(dZ1, axis=0)\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "        # --- Update Weights and Biases (Gradient Descent) ---\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "\n",
    "    def calculate_loss(self, y, output):\n",
    "        \"\"\"Calculates the Mean Squared Error (MSE) loss.\"\"\"\n",
    "        return np.mean((y.reshape(-1, 1) - output) ** 2)\n",
    "\n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01):\n",
    "        \"\"\"Trains the neural network for a specified number of epochs.\"\"\"\n",
    "        print(f\"\\nStarting training for {epochs} epochs with LR={learning_rate}...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward Pass\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Calculate Loss (for monitoring)\n",
    "            loss = self.calculate_loss(y, output)\n",
    "            \n",
    "            # Backward Pass (Updates weights and biases)\n",
    "            self.backward(X, y, output, learning_rate)\n",
    "\n",
    "            if epoch % (epochs // 10) == 0 or epoch == epochs - 1:\n",
    "                print(f\"Epoch {epoch}/{epochs} - Loss: {loss:.6f}\")\n",
    "\n",
    "        print(\"Training complete.\")\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Makes predictions using the trained network.\"\"\"\n",
    "        # Run forward pass and apply threshold\n",
    "        output = self.forward(X)\n",
    "        return (output >= 0.5).astype(int)\n",
    "\n",
    "# --- Demonstration and Execution ---\n",
    "\n",
    "# 1. Define Sample Data (XOR-like problem, 4 samples, 2 features)\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "# True labels (XOR result)\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# 2. Initialize the Network\n",
    "input_features = X.shape[1]  # 2 features\n",
    "hidden_neurons = 4           # Choose a size for the hidden layer\n",
    "output_classes = 1           # Binary classification (0 or 1)\n",
    "\n",
    "nn = SimpleNeuralNetwork(input_features, hidden_neurons, output_classes)\n",
    "\n",
    "# 3. Train the Network\n",
    "epochs = 50000\n",
    "learning_rate = 0.1\n",
    "final_output = nn.train(X, y, epochs=epochs, learning_rate=learning_rate)\n",
    "\n",
    "# 4. Evaluate and Print Results\n",
    "predictions = nn.predict(X)\n",
    "\n",
    "print(\"\\n--- Final Results ---\")\n",
    "print(\"Input (X):\\n\", X)\n",
    "print(\"True Labels (y):\\n\", y)\n",
    "print(\"Predicted Probabilities (A2):\\n\", final_output.flatten())\n",
    "print(\"Final Predictions:\\n\", predictions.flatten())\n",
    "\n",
    "accuracy = np.mean(predictions.flatten() == y) * 100\n",
    "print(f\"\\nFinal Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f321263d-4552-40a2-b87f-cbbad8f935fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_env)",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
